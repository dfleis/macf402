% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,mathtools}
\usepackage{dsfont} % for indicator function \mathds 1
\usepackage{tikz,pgf,pgfplots}
\usepackage{enumerate} 
\usepackage{graphicx,float} % figures
\usepackage{csvsimple,longtable,booktabs} % load csv as a table
\usepackage{listings,color} % for code snippets

\newenvironment{theorem}[2][Theorem:]{\begin{trivlist} %% Theorem Environment
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newtheorem{definition}{Definition}
\let\olddefinition\definition
\renewcommand{\definition}{\olddefinition\normalfont}
\newtheorem{lemma}{Lemma}
\let\oldlemma\lemma
\renewcommand{\lemma}{\oldlemma\normalfont}
\newtheorem{proposition}{Proposition}
\let\oldproposition\proposition
\renewcommand{\proposition}{\oldproposition\normalfont}
\newtheorem{corollary}{Corollary}
\let\oldcorollary\corollary
\renewcommand{\corollary}{\oldcorollary\normalfont}

\newcommand\norm[1]{\left\lVert#1\right\rVert} % \norm command 

\pgfmathsetseed{26} %% done for plotting stochastic processes
\newcommand{\Emmett}[5]{% points, advance, rand factor, options, end label
\draw[#4] (0,0)
\foreach \x in {1,...,#1}
{   -- ++(#2,rand*#3+0.001)
}
node[right] {#5};
}

%% set noindent by default and define indent to be the standard indent length
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Mathematical \& Computational Finance II\\Lecture Notes}
\author{Basics of Stochastic Processes}
\date{September 24 2015 \\ Last update: \today{}}
\maketitle

\begin{section}{Brownian Motion}

\indent Given a Brownian motion on a probability space $(\Omega, \mathcal F, \mathbb P)$ equipped with a filtration $(\mathcal F_t)_{t\geq0}$, a $d$-dimensional Brownian motion starting at the origin (i.e. a standard Brownian motion, $B_0 = \hat{0}$) is a stochastic process such that

\begin{enumerate}
	\item $B_t$ has independent increments. That is, for $0 = t_0 \leq t_1 \leq \cdots \leq t_n$, we have
		\begin{equation*}
			B_{t_1} - B_{t_0}, B_{t_2} - B_{t_1}, \cdots, B_{t_n} - B_{t_{n-1}}
		\end{equation*}
		are independent random variables. So, any partition has independent intervals.
	\item For $0 \leq s \leq t, B_t - B_s \sim N(0, (t - s)\mathbb I^d)$, where $\mathbb I^d$ is the $d$-dimensional identity matrix.
	\item $\mathbb P(\{\omega : B_0(\omega) = 0 \text{ and } t \mapsto B_t(\omega) \text{ is continuous}\}) = 1$ (i.e. our process $B_t$ is almost surely continuous).
\end{enumerate}

\subsection{Implications}

We have
\begin{equation*}
	\mathbb P(B_t - B_s \leq x) = \frac{1}{\sqrt{2\pi(t-s)}}\int^x_{-\infty} e^{-\frac{1}{2}\frac{z^2}{t-s}}\,dz
\end{equation*}

\indent This gives us that $B_t - B_s$ is independent of the ``natural filtration'' generated by the Brownian motion up to $s$. That is, $B_t - B_s$ is independent of $\mathcal F^B_s = \sigma(B_v - B_0: 0 \leq v \leq s) = \sigma(B(v): 0 \leq v \leq s)$. For $f$ is sufficiently ``nice''\footnote{Lebesgue measurable} then we have
\begin{equation*}
	\mathbb E[f(B_t - B_s)|\mathcal F^B_s] = \mathbb E[f(B_t - B_s)]
\end{equation*}

In particular, for $0 \leq s \leq t$,
\begin{align*}
	\mathbb E[B_t | \mathcal F^B_s] &= \mathbb E[B_s + (B_t - B_s)|\mathcal F^B_s] \\
	&= B_s + \mathbb E[B_t - B_s|\mathcal F^B_s] \quad \text{(taking out what is known)} \\
	&= B_s + \mathbb E[B_t - B_s] \quad \text{(independent increments)}
\end{align*}

But $B_t - B_s \sim N(0,(t - s)\mathbb I)$ so
\begin{align*}
	\mathbb E[B_t | \mathcal F^B_s] &= B_s + 0 \\
	&= B_s
\end{align*}

\indent $\therefore$ we say that our process $(B_t)_{t\geq0}$ is a $(\mathcal F^B_t, \mathbb P)$-martingale. We can interpret a $(\mathcal F^B_t, \mathbb P)$-martingale as ``our best estimate for the future state of this process is what it is currently at''.

\subsection{Martingales}
\begin{definition} Let $(F_t)_{t\geq0}$ be a filtration\footnote{Note that each $\mathcal F_i$ is a $\sigma$-algebra and that $\mathcal F_s \subseteq \mathcal F_t$ for $s \leq t$.}. A stochastic process $(M_t)_{t\geq0}$ is an $(\mathcal F_t, \mathbb P)$-martingale if
\begin{enumerate}
	\item $M_t$ is $\mathcal F_t$-measurable $\forall\,t\geq 0$. ``$M_t \in \mathcal F_t $'' $\iff$ ``$\mathcal F_t$-adapted''.
	\item $\mathbb E[|M_t|] < \infty, \forall\,t\geq0$ (i.e. the process is integrable).
	\item ``The Martingale Property'' $\mathbb E[M_t|\mathcal F_s] = M_s, \forall\,s\leq t$
\end{enumerate}
\end{definition}

\subsubsection{``We can create other martingales from Brownian motion''}

\begin{lemma} Suppose $(B_t)_{t\geq0}$ is a Brownian motion.
\begin{enumerate}
	\item $(B_t)_{t\geq0}$ is a $(\mathcal F^B_t, \mathbb P)$-martingale.
	\item ``Important property for later''\footnote{Maybe when we introduce It\^{o} Calculus?} $B^2_t - t = M_t$ is a martingale.
	\item For any $\sigma>0$, $N_t = e^{\sigma B_t - \frac{\sigma^2}{2}t}$ is a $(\mathcal F_t, \mathbb P)$-martingale.
\end{enumerate}

\begin{proof} {\em (Partial) Proof that $B^2_t - t = M_t$ is a martingale.} Note
\begin{align*}
	\mathbb E[B_t^2 - B_s^2|\mathcal F^B_s] &= \mathbb E[(B_t - B_s)^2 + 2B_s(B_t - B_s)]\mathcal F^B_s] \\
	&=  \mathbb E[(B_t - B_s)^2|\mathcal F^B_s] + 2\mathbb E[B_s(B_t - B_s)]\mathcal F^B_s] \quad \text{(by linearity)} \\
	&= \mathbb E[(B_t - B_s)^2|\mathcal F^B_s] + 2B_s\mathbb E[(B_t - B_s)]\mathcal F^B_s] \quad \text{(taking out what is known)} \\
	&= \mathbb E[(B_t - B_s)^2] + 2B_s\mathbb E[(B_t - B_s)] \quad \text{(independent intervals)} \\
	&= \mathbb E[(B_t - B_s)^2]  + 0 \quad \text{(intervals are normally distributed with mean 0)}
\end{align*}

Recall
\begin{align*}
	\text{Var}[X] &= \mathbb E[X^2] - \mathbb E^2[X] \\
	\implies \mathbb E[X^2] &= \text{Var}[X] + \mathbb E^2[X]
\end{align*}

So substituting $X$ with $B_t - B_s$ we have
\begin{align*}
	\mathbb E[B_t^2 - B_s^2|\mathcal F^B_s] &= \mathbb E[(B_t - B_s)^2] \\
	&= \text{Var}[B_t - B_s] + \mathbb E^2[B_t - B_s] \\
	&= (t - s) + 0^2 \quad \text{(intervals are normally distributed with variance (t - s))} \\
	&= t - s
\end{align*}

However, note that $B^2_s \in \mathcal F^B_s$ so
\begin{equation*}
	\mathbb E[B^2_t - B^2_s|\mathcal F^B_s] = \mathbb E[B^2_t|\mathcal F^B_s] - B^2_s \quad \text{(taking out what is known)}
\end{equation*}

Hence
\begin{align*}
	&\mathbb E[B^2_t|\mathcal F^B_s] - B^2_s = t - s \\
	\implies &\mathbb E[B^2_t|\mathcal F^B_s]  - t = B^2_s - s \\
	\implies &\mathbb E[B^2_t - t | \mathcal F^B_s] = B^2_s - s
\end{align*}

or equivalently
\begin{equation*}
	\mathbb E[M_t|\mathcal F^B_s] = M_s
\end{equation*}
\end{proof}
\end{lemma}

Prove that any $\sigma>0$, $N_t = e^{\sigma B_t - \frac{\sigma^2}{2}t}$ is a $(\mathcal F_t, \mathbb P)$-martingale.

\begin{proof} Recall that if $Z\sim N(0,1)$ then for $\lambda > 0$
\begin{equation*}
	\mathbb E[e^{\lambda Z}] = m_Z(\lambda) = e^{\frac{\lambda^2}{2}}
\end{equation*}

So
\begin{align*}
	\mathbb E[N_t|\mathcal F^B_s]  &= \mathbb E[e^{\sigma B_t - \frac{\sigma^2}{2}t}|\mathcal F^B_s] \\
	&= \mathbb E[e^{\sigma B_t -\sigma B_s + \sigma B_s - \frac{\sigma^2}{2}t}|\mathcal F^B_s] = \mathbb E[e^{\sigma(B_t - B_s)}e^{\sigma B_s - \frac{\sigma^2}{2}t}|\mathcal F^B_s] \\
	&=  \mathbb E[e^{\sigma(B_t - B_s)}|\mathcal F^B_s]e^{\sigma B_s - \frac{\sigma^2}{2}t} \quad \text{(taking out what is known)} \\
	&= \mathbb E[e^{\sigma(B_t - B_s)}]e^{\sigma B_s - \frac{\sigma^2}{2}t} \quad \text{(independent increments)}
\end{align*}

But $\mathbb E[e^{\sigma(B_t - B_s)}]$ is the moment generating function\footnote{If $X \sim N(\mu, \sigma^2)$ then the MGF of $X$, denoted $M_x(t) = \mathbb E[ e^{tX} ] = e^{\mu t + \frac{\sigma^2}{2}t^2}$.} of $N(0, t - s)$. So,
\begin{align*}
	\mathbb E[N_t|\mathcal F^B_s] &= e^{\frac{\sigma^2}{2}(t-s)}e^{\sigma B_s - \frac{\sigma^2}{2}t} \\
	&= e^{\frac{\sigma^2}{2}(t-s) + \sigma B_s - \frac{\sigma^2}{2}t} \\
	&= e^{\frac{\sigma^2}{2}t - \frac{\sigma^2}{2}s + \sigma B_s - \frac{\sigma^2}{2}t} \\
	&= e^{\sigma B_s - \frac{\sigma^2}{2}s} \\
	&= N_s
\end{align*}
\end{proof}

\subsection{Geometric Brownian Motion}

Our first model for a stock price will be
\begin{equation*}
	S_t = S_0 e^{\mu t \sigma B_t - \frac{1}{2}\sigma^2 t}
\end{equation*}

\indent $S_t$ is \underline{Geometric Brownian motion} (GBM) where $\mu$ is our drift parameter and $\sigma$ is our volatility parameter. Why is this so? Note
\begin{align*}
	\ln \left( \frac{S_t}{S_0} \right) &= \mu t + \sigma B_t - \frac{1}{2}\sigma^2 t \\
	&= \left( \mu - \frac{1}{2}\sigma^2 \right)t + \sigma B_t 
\end{align*}

\indent  We see that the log returns have a normal distribution, $\ln\big(\frac{S_t}{S_0}\big) \sim N(\mu - \frac{1}{2}\sigma^2), \sigma^2)$, and so returns (and thereby asset prices $S_t$) have a log-normal distribution.\footnote{A random variable $X$ is lognormally distributed if, for $Y\sim N(\mu, \sigma^2)$, we have $X=e^Y$.} \\

\indent Recall that for $Z\sim N(0,1)$ then $cZ\sim N(0, c^2)$. So to generate a GBM with variance $t - s = \Delta t$ first generate $Z\sim N(0,1)$ and set $B_{t_n} = B_{t_{n-1}} + \sqrt{\Delta t}Z$.

\begin{theorem}{Brownian motion is continuous everywhere but differentiable nowhere} \hfill
\begin{proof} {\em (Proof outline)} Consider a differentiable function $f$. Then for some partition of some interval $\pi = \{0 = t_0 \leq t_1 \leq \cdots \leq t_n = T\}$ of $[0,T]$ define the \underline{total variation} of $f$ over $[0,T]$ to be,
\begin{equation*}
	V(f) = \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0} |f(t_{i+1} - f(t_i)|
\end{equation*}

\indent Where $|\pi| = \max(|t_{i+1} - t_i|)$ and can be interpreted as ``the width of the mesh of the partition''. If $f$ is differentiable over $[0,T]$ then by the mean value theorem we have
\begin{equation*}
	f(t_{i+1}) - f(t_i) = f'(t^*_i)(t_{i+1} - t_i)
\end{equation*}

For some $t^*_i \in (t_i, t_{i+1})$. So,
\begin{align*}
	V(f) &= \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0} |f(t_{i+1} - f(t_i)| \\
	&= \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0}|f'(t^*_i)|(t_{i+1} - t_i) \\
	&= \int^T_0 \left| f'(u) \right| du \quad \text{(not necessarily finite)}
\end{align*}

\indent We say that $f$ is of \underline{bounded variation} on $[0,T]$ if $V(f) < \infty$. Now, define \underline{quadratic variation} to be
\begin{equation*}
	QV(f) = \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0} |f(t_{i+1}) - f(t_i)|^2
\end{equation*}

\indent Why do we care about this? If $f$ is twice differentiable we can use the mean value theorem again
\begin{align*}
	QV(f) &= \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0} |f(t_{i+1}) - f(t_i)|^2 \\
	&= \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0}|f'(t^*_i)|^2(t_{i+1} - t_i)^2 \\
	&= \lim_{|\pi|\to 0} |\pi| \sum^{n-1}_{i = 0}|f'(t^*_i)|^2(t_{i+1} - t_i) \\
	&= (0) \lim_{|\pi|\to 0} \sum^{n-1}_{i = 0}|f'(t^*_i)|^2(t_{i+1} - t_i) \quad \text{(I'm pretty sure this is a handwavy part)} \\
	&= 0
\end{align*}

\indent So if $f$ is differentiable on $[0,T]$ then $QV(f) = 0$ and so any function with bounded variation must have 0 quadratic variation.

\begin{theorem}{The quadratic variation of a Brownian motion is nonzero} That is, if $B_t$ is a standard one-dimensional Brownian motion then $QV(B_t) = T$ over the interval $[0,T]$. Therefore, the sample paths $t \mapsto B_t(\omega)$ for fixed $\omega$ of a Brownian motion have infinite total variation (since $QV(B_t) \neq 0$).

\begin{proof} {\em (We will only begin the proof)}. Write $D_k(\omega) = B_{t_{k+1}}(\omega) - B_{t_{k}}(\omega)$ for fixed $\omega$. Then, the quadratic variation of $t \mapsto B_t(\omega)$ is
\begin{equation*}
	Q_\pi(\omega) = \sum^{n-1}_{k = 0} \left( D_k(\omega) \right)^2
\end{equation*}

Note we can write $T$ as the telescopic sum (with $t_n \equiv T$ and $t_0 \equiv 0$)
\begin{equation*}
	T = \sum^{n - 1}_{k = 0} t_{k + 1} - t_k
\end{equation*}

so
\begin{align*}
	Q_\pi - T &= \sum^{n - 1}_{k = 0} \left( D_k \right)^2  \sum^{n - 1}_{k = 0}  t_{k+1} - t_k \\
	&= \sum^{n - 1}_{k = 0} \left( D_k \right)^2 - (t_{k + 1} - t_k)
\end{align*}

But we have $D_k = B_{t_{k + 1}} - B_{t_k} \sim N(0, t_{k + 1} - t_k)$, so
\begin{equation*}
	\mathbb E \left[ \left( D_k \right)^2 \right] = t_{k + 1} - t_k
\end{equation*}

hence
\begin{equation*}
	\mathbb E[\left( D_k \right)^2 - (t_{k+1} - t_k)] = 0
\end{equation*}

Therefore
\begin{equation*}
	\mathbb E \left[ Q_\pi - T \right] = \mathbb E \left[ \sum^{n - 1}_{k = 0} \left( D_k \right)^2 - (t_{k + 1} - t_k) \right] = 0
\end{equation*}

\indent Now, for $j\neq k$, we note that $(D_j)^2 - (t_{j+1} - t_j)$ and $(D_k)^2 - (t_{k+1} - t_k)$ are independent increments of Brownian motion. We wish to show that $(Q_\pi - T)^2$ has expectation 0. However, have just shown that
\begin{equation*}
	\mathbb E[Q_\pi - T] = 0
\end{equation*}

and note that $\mathbb E[(Q_\pi - T)^2]$ has some terms that look like 
\begin{align*}
	&\mathbb E[(D^2_j - (t_{j+1} - t_j)) \cdot (D^2_k - (t_{k+1} - t_k))] \quad \text{but by independence,} \\
	&= \mathbb E[0] \cdot \mathbb E[0] = 0
\end{align*}

\indent So we may discard these terms for $j \neq k$ in our sum and consider the expectation of terms such that $j = k$
\begin{equation*}
	\mathbb E[(Q_\pi - T)^2] = \mathbb E[\sum^{n - 1}_{k = 0}(D^4_k - 2D^2_k(t_{k+1} - t_k) + (t_{k+1} - t_k)^2)] 
\end{equation*}

We now rely on a result\footnote{Proven in a later lecture (I don't think (1) is ever proven?).}
\begin{enumerate}
	\item $\mathbb E[B_t B_s] = \min(t,s)$
	\item $\mathbb E[(B_t-B_s)^m] = 
		\begin{cases}
		0 & \text{if m odd} \\
		1\cdot3\cdot5\cdots(m-3)\cdot(m-1)(t-s)^{^m/_2} & \text{for m even}
		\end{cases}$
\end{enumerate}

then
\begin{align*}
	\mathbb E[(Q_\pi - T)^2] &= \sum^{n - 1}_{k = 0} \Big( \mathbb E[D^4_k] - 2\mathbb E[D^2_k(t_{k+1} - t_k)] + \mathbb E[(t_{k+1} - t_k)^2] \Big) \\
	&=  \sum^{n - 1}_{k = 0} \Big( \mathbb E[D^4_k] - 2(t_{k+1} - t_k)\mathbb E[D^2_k] + (t_{k+1} - t_k)^2 \Big) \\
	&=  \sum^{n - 1}_{k = 0} \Big( \mathbb E[(B_{t_{k + 1}} - B_{t_k})^4] - 2(t_{k+1} - t_k)\mathbb E[(B_{t_{k + 1}} - B_{t_k})^2] + (t_{k+1} - t_k)^2 \Big) \\
	&= \sum^{n - 1}_{k = 0} \Big( 3(t_{k+1} - t_k)^2 - 2(t_{k+1} - t_k)^2 + (t_{k+1} - t_k)^2 \Big) \\ 
	&= 2\sum^{n - 1}_{k = 0}(t_{k+1} - t_k)^2 \leq 2|\pi|T
\end{align*}
So,
\begin{align*}
	\lim_{|\pi|\to0} \text{Var}(Q_\pi - T) &= 0 \quad \text{and} \\
	\mathbb E[Q_\pi - T] &= 0 \\
	\implies \lim_{|\pi|\to0} (Q_\pi - T) &= 0 \quad \text {a.s., so} \\
	\lim_{|\pi|\to0} Q_\pi &= T \quad \text{a.s.}
\end{align*}

\end{proof}
\end{theorem}

\indent We have just shown that the quadratic variation of is nonzero, thus we may state that Brownian motion has unbounded total variation. Therefore, Brownian does not meet the criteria for being differentiable and so we say that Brownian motion is continuous everywhere but differentiable nowhere.

\end{proof}
\end{theorem}

\end{section}





























































\end{document}