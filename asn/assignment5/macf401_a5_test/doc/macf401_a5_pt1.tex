% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{bm} % bold in mathmode \bm
\usepackage{amsmath,amsthm,amssymb,mathtools}
\usepackage{dsfont} % for indicator function \mathds 1
\usepackage{tikz,pgf,pgfplots}
\usepackage{enumerate} 
\usepackage[multiple]{footmisc} % for an adjascent footnote
\usepackage{graphicx,float} % figures

\newtheorem{definition}{Definition}
\let\olddefinition\definition
\renewcommand{\definition}{\olddefinition\normalfont}
\newtheorem{lemma}{Lemma}
\let\oldlemma\lemma
\renewcommand{\lemma}{\oldlemma\normalfont}
\newtheorem{proposition}{Proposition}
\let\oldproposition\proposition
\renewcommand{\proposition}{\oldproposition\normalfont}
\newtheorem{corollary}{Corollary}
\let\oldcorollary\corollary
\renewcommand{\corollary}{\oldcorollary\normalfont}
\newtheorem{theorem}{Theorem}
\let\oldtheorem\theorem
\renewcommand{\theorem}{\oldtheorem\normalfont}

\newcommand\norm[1]{\left\lVert#1\right\rVert} % \norm command 

%%% PLOTTING PARAMETERS
\tikzstyle{bag} = [text width=7em, text centered] %binomial tree node width
\tikzstyle{end} = []
%%%

%% set noindent by default and define indent to be the standard indent length
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\newcommand*{\vv}[1]{\vec{\mkern0mu#1}} % \vec command

%% DAVIDS MACRO KIT %%
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\renewcommand{\P}{\mathbb P}
\newcommand{\Q}{\mathbb Q}
\newcommand{\E}{\mathbb E}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\indist}{\,{\buildrel \mathcal D \over \sim}\,}

\newcommand{\bigtau}{\text{{\large $\bm \tau$}}}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Assignment 4}
\author{David Fleischer -- 27101449\\ 
MACF 401 - Mathematical \& Computational Finance I}
\date{Due: April 4 2016 \\ Last update: \today{}}
\maketitle

\section*{Part I}

{\bf Solution 5.2:} \\

%%
%% 5.2 (i)
%%
{\bf (i)} We have
\begin{align*}
	f(\sigma) &= pe^\sigma + q^{-\sigma} \\
	&= pe^\sigma + (1 - p)e^{-\sigma} \\
	&= pe^\sigma + e^{-\sigma} - pe^{-\sigma} \\
	&= p \left( e^\sigma - e^{-\sigma} \right) + e^{-\sigma} \\
\end{align*}

However, note that $\forall\,x \in \R$ we have $(x - 1)^2 > 0$, hence
\begin{align*}
	(x - 1)^2 &= x^2 - 2x + 1 \geq 0 \\
	\implies x^2 + 1 &\geq 2 \\
	\implies \frac{ x^2 + 1 }{ x } &\geq 2 \\
	\implies x + x^{-1} &\geq 2
\end{align*}

Thus $(e^\sigma - e^{-\sigma}) \geq 2$ and since $p > \frac{1}{2}$ we find
\begin{equation*}
	p \left( e^\sigma - e^{-\sigma} \right) \geq 1
\end{equation*}

This, together with the final term $e^{-\sigma} > 0$ gives us that
\begin{equation*}
	f(\sigma) = p \left( e^\sigma - e^{-\sigma} \right) + e^{-\sigma} > 1
\end{equation*}

as desired.

%%
%% 5.2 (ii)
%%
{\bf (ii)} By definition we have that $M_n = \sum^n_{j = 1} X_j$ depends on only the first $n$ coin tosses $\omega_1\cdots\omega_n$. Note that $\left( \frac{1}{f(\sigma)} \right)^n$ is deterministic we find that $S_n = e^{\sigma M_n} \left( \frac{1}{f(\sigma)} \right)^n$ is adapted since a measurable function of an adapted process is itself adapted. \\

Now, to confirm the martingale property:
\begin{align*}
	\E_n \left[ S_{n + 1} \right] &= \E_n \left[ e^{\sigma M_{n + 1}} \left( \frac{1}{f(\sigma)} \right)^{n + 1} \right] \\
	&= \E_n \left[ e^{\sigma (M_n + X_{n + 1})} \left( \frac{1}{f(\sigma)} \right)^{n + 1} \right] \quad \text{(by definition of $M_{n + 1}$)} \\
	&= \E_n \left[ e^{\sigma M_n} \left( \frac{1}{f(\sigma)} \right)^n \left( \frac{1}{f(\sigma)} \right) e^{\sigma X_{n + 1}} \right] \\
	&= \E_n \left[ \frac{S_n}{f(\sigma)} e^{\sigma X_{n + 1}} \right] \quad \text{(by definition of $S_n$)} \\
	&= \frac{ S_n }{ f(\sigma) } \E_n \left[ e^{\sigma X_{n + 1}} \right] \quad \text{(adaptedness of $\frac{S_n}{f(\sigma)}$ to the first $n$ coin tosses)} \\
	&= \frac{ S_n }{ f(\sigma) } \left( pe^{\sigma \cdot (1)} + qe^{\sigma \cdot(-1)} \right) \quad \text{(by the independence lemma)} \\
	&= \frac{ S_n }{ f(\sigma) } f(\sigma) \\
	&= S_n
\end{align*}

\indent Therefore, since $S_n$ is both adapted and satisfies the martingale property we have that $S_n$ is indeed a martingale, as desired. \\

%%
%% 5.2 (iii)
%%
{\bf (iii)} Applying the Optional Sampling Theorem we find that the stopped process $S_{n\land\tau_1}$ must be a martingale. So
\begin{align*}
	\E_0 \left[ S_{n\land\tau_1} \right] &= \E_0 \left[ e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} \right] \\
	&= S_{0\land\tau_1} \quad \text{(by the martingale property)} \\
	&= S_0 \quad \text{(since $\tau_1 > 0$)} \\
	&= e^{\sigma M_0} \left( \frac{1}{f(\sigma)} \right)^0 \\
	&= 1
\end{align*}

Note
\begin{equation*}
	\lim_{n\to\infty} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} =
	\begin{cases}
		\left( \frac{1}{f(\sigma)} \right)^{\tau_1} & \text{if } \tau_1 < \infty \\
		0 & \text{if } \tau_1 = \infty
	\end{cases}
\end{equation*}

By the definition of $M_{n\land\tau_1}$ we note that
\begin{equation*}
	M_{n\land\tau_1} \leq 1
\end{equation*}

Hence
\begin{equation*}
	0 \leq e^{\sigma M_{n\land\tau_1}} \leq e^\sigma
\end{equation*}

Now, considering first $\tau_1 < \infty$
\begin{align*}
	\lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} &= \lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \lim_{n\to\infty} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} \\
	&= e^\sigma \left( \frac{1}{f(\sigma)} \right)^{\tau_1}
\end{align*}

and for the case of $\tau_1 = \infty$ we use our result that $e^{\sigma M_n}$ is bound above and below, so
\begin{align*}
	\lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} &= \lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \lim_{n\to\infty} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} \\
	&= \lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \cdot 0 \\
	&= 0
\end{align*}

Therefore, we may combine both cases as
\begin{equation*}
\lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} = \mathds 1_{\{ \tau_1 < \infty \}} e^\sigma \left( \frac{1}{f(\sigma)} \right)^{\tau_1}
\end{equation*}

\indent Now we wish to take the limit of the expectation of the stopped process $S_{n\land\tau_1}$ as $n \to \infty$. However, we have already shown that $\E_0[S_{n\land\tau_1}] = S_{0 \land \tau_1} = S_0 = 1$. So
\begin{align*}
	\lim_{n\to\infty} \E_0[S_{n\land\tau_1}] &= \lim_{n\to\infty} 1 \\
	&= 1
\end{align*}

Thus
\begin{align*}
	1 &= \lim_{n\to\infty} \E_0 \left[ S_{n\land\tau_1} \right] \\
	&= \E_0 \left[ \lim_{n\to\infty} S_{n\land\tau_1} \right] \quad \text{(Dominated Convergence)} \\
	&= \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} e^\sigma \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right]  \\
	\implies e^{-\sigma} &= \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] 
\end{align*}

and taking the limit as $\sigma \downarrow 0$
\begin{align*}
	\lim_{\sigma\downarrow 0} e^{-\sigma} &= \lim_{\sigma\downarrow 0} \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	\implies 1 &= \lim_{\sigma \downarrow 0} \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	&= \E_0 \left[ \lim_{\sigma\downarrow 0} \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \quad \text{(Dominated Convergence)} \\
	&= \E_0 \left[ \mathds 1_{\{\tau_1 < \infty\}} \lim_{\sigma \downarrow 0} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	&= \E_0 \left[ \mathds 1_{\{\tau_1 < \infty \}} \right] \\
	&= \P \left( \{\tau_1 < \infty\} \right)
\end{align*}

as desired. \\

%%
%% 5.2 (iv)
%%
{\bf (iv)} Let $\alpha \in (0,1)$. We will first solve for the $\sigma$ satisfying
\begin{align*}
	\alpha &= \frac{1}{f(\sigma)} \\
	&= \frac{1}{ pe^{\sigma} + qe^{-\sigma} } \\
	\implies \alpha \left( pe^\sigma + qe^{-\sigma} \right) &= 1 \\
	\implies \alpha p e^\sigma + \alpha q e^{-\sigma} &= 1 \\
	\implies \alpha p + \alpha q\left( e^{-\sigma} \right)^2 &= e^{-\sigma} \\
	\implies \alpha q \left( e^{-\sigma} \right)^2 - e^{-\sigma} + \alpha p &= 0 \\ 
	\implies e^{-\sigma} &= \frac{ 1 \pm \sqrt{1 - 4\alpha^2 q }}{ 2\alpha pq } 
\end{align*}

We require $\sigma > 0$ so then $0 < e^{-\sigma} < 1$. For this purpose we take the negative root
\begin{equation*}
	e^{-\sigma} = \frac{ 1 - \sqrt{1 - 4\alpha^2 pq}}{ 2\alpha q } 
\end{equation*}

and from {\bf (iii)} we find
\begin{align*}
	e^{-\sigma} &= \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	\implies \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] &= \frac{ 1 - \sqrt{1 - 4\alpha^2 pq} }{ 2\alpha q } 
\end{align*}

Since we initially performed the substitution $\alpha = \frac{1}{f(\sigma)}$ we may write
\begin{equation*}
	\E \left[ \mathds 1_{\{\tau_1 < \infty \}} \alpha^{\tau_1} \right] = \frac{ 1 - \sqrt{1 - 4\alpha^2 pq } }{ 2\alpha q } 
\end{equation*}

Noting that
\begin{equation*}
	\E \left[\alpha^{\tau_1} \right] = \E \left[ \mathds 1_{\{\tau_1 = \infty \}} \alpha^{\tau_1} \right] + \E \left[ \mathds 1_{\{\tau_1 < \infty \}} \alpha^{\tau_1} \right] 
\end{equation*}

and since $\alpha \in (0,1)$ we find $\mathds 1_{\{\tau_1 = \infty \}} \alpha^{\tau_1} = 0$. Therefore, we may conclude with
\begin{equation*}
	\E \left[ \alpha^{\tau_1} \right] = \E \left[ \mathds 1_{\{\tau_1 < \infty \}} \alpha^{\tau_1} \right] = \frac{ 1 - \sqrt{1 - 4\alpha^2 pq } }{ 2\alpha q } 
\end{equation*}


as desired. \\

%%
%% 5.2 (v)
%%
{\bf (v)} By the Dominated Convergence Theorem write
\begin{align*}
	\E \left[ \tau_1 \alpha^{\tau_1 - 1} \right] &= \frac{ \partial }{ \partial \alpha } \E \left[ \alpha^{\tau_1} \right] \\
	&= \frac{ \partial }{ \partial \alpha }  \frac{ 1 - \sqrt{1 - 4\alpha^2 pq } }{ 2\alpha q }  \quad \text{(from {\bf (iv)})} \\
	&= \frac{ 1 - \sqrt{1 - 4\alpha^2 pq} }{ 2\alpha^2 q \sqrt{1 - 4\alpha^2 pq} }
\end{align*}

Since we had defined $\alpha \in (0,1)$ we must take the limit as $\alpha \uparrow 1$
\begin{align*}
	\lim_{\alpha \uparrow 1} \E \left[ \tau_1 \alpha^{\tau_1 - 1} \right] &= \lim_{\alpha \uparrow 1} \frac{ 1 - \sqrt{1 - 4\alpha^2 pq} }{ 2\alpha^2 q \sqrt{1 - 4\alpha^2 pq} } \\
	&= \frac{ 1 - \sqrt{1 - 4 pq} }{ 2 q \sqrt{1 - 4 pq} } \\
	&= \frac{ 1 - \sqrt{1 - 4 p ( 1 - p ) } }{ 2 (1 - p) \sqrt{1 - 4 p (1 - p)} } \\
	&= \frac{ 1 - \sqrt{ 1 - 4p + 4p^2 }}{ 2 (1 - p) \sqrt{ 1 - 4p + 4p^2 } } \\
	&= \frac{ 1 - \sqrt{(1 - 2p)^2} }{ 2(1 - p) \sqrt{ (1 - 2p)^2 } } \\
	&= \frac{1 - |1 - 2p| }{ 2(1 - p)|1 - 2p| } \\
	&= \frac{1 - (2p - 1) }{ 2(1 - p)(2p - 1) } \quad \text{since } 1 - 2p < 0 \text{ for $\frac{1}{2} < p < 1$} \\
	&= \frac{ 2 - 2p }{ 2(1 - p)(2p - 1) } \\
	&= \frac{1}{2p - 1} = \frac{1}{p + p - 1} = \frac{1}{p - q} \quad \text{(I think the final expression is the nicest)}
\end{align*}

Hence
\begin{align*}
	\frac{1}{p - q} &= \lim_{\alpha \uparrow 1} \E \left[ \tau_1 \alpha^{\tau_1 - 1} \right] \\
	&= \E \left[ \lim_{\alpha \uparrow 1} \tau_1 \alpha^{\tau_1 - 1} \right] \quad \text{(Dominated Convergence)} \\
	&= \E \left[ \tau_1  \right]
\end{align*}

as desired. \\

\newpage
{\bf Solution 5.3}: 

%%
%% 5.3 (i)
%%
{\bf (i)} With the substitution $x = e^{\sigma_0}$ we solve
\begin{align*}
	1 &= px + \frac{q}{x} \\
	&= \frac{px^2 + q}{x} \\
	\implies x &= px^2 + q \\
	\implies px^2 - x + q &= 0 \\
	\implies x &= \frac{ 1 \pm \sqrt{1 - 4pq} }{ 2p } \\
	&= \frac{ 1 \pm \sqrt{1 - 4p(1 - p) }}{ 2p } \\
	&= \frac{ 1 \pm \sqrt{(1 - 2p)^2} }{ 2p } \\
	&= \frac{ 1 \pm (1 - 2p) }{ 2p }
\end{align*}

\indent Clearly we must have $x = e^\sigma > 0$ for all $\sigma \in \R$, so we are required to take the positive term. This yields
\begin{align*}
	x &= \frac{ 1 + 1 - 2p }{ 2p } \\
	&= \frac{1 - p}{ p } \\
	&= \frac{q}{p} \\
	\implies e^{\sigma_0} &= \frac{q}{p} \\
	\implies \sigma_0 &= \log \frac{q}{p}
\end{align*}

\indent Since $q > p$ we have that $\frac{q}{p} > 1 \implies \sigma_0 = \log \frac{q}{p} > 1 > 0$, which satisfies our positivity criteria. That is, we have found $\sigma_0 > 0$ such that $f(\sigma_0) = 1$ since
\begin{align*}
	f(\sigma_0) &= pe^{\sigma_0} + qe^{-\sigma_0} \\
	&= pe^{log \frac{q}{p} } + qe^{-\log \frac{q}{p} } \\
	&= p \cdot \frac{q}{p} + q \cdot \frac{p}{q} \\ 
	&= q + p = 1
\end{align*}

\indent We wish now to confirm that $f(\sigma) > 1$ for all $\sigma > \sigma_0 = \log \frac{q}{p}$. To this end, again let $x = e^\sigma$ and calculate
\begin{align*}
	1 &\leq px + \frac{q}{x} \\
	1 &\leq \frac{px^2 + q}{x} \\
	x &\leq px^2 + q \\
	0 &\leq px^2 - x + q \\
	0 &\leq (x - 1)(px - q)
\end{align*}

which is satisfied when $x \geq \frac{q}{p} \implies e^\sigma \geq \frac{q}{p} \implies \sigma \geq \log \frac{q}{p} = \sigma_0$, as desired.\\

%%
%% 5.3 (ii)
%%
{\bf (ii)} We go through the same process as in {\bf (5.2)}. Defining the process $S_{n} = e^{\sigma M_n} \left( \frac{1}{ f(\sigma) } \right)^n$ we see by the same argument that $S_n$ is an adapted process since it is a measurable function of $M_n$, which is itself a function of the first $n$ coin tosses. To confirm the martingale property we have

Now, to confirm the martingale property:
\begin{align*}
	\E_n \left[ S_{n + 1} \right] &= \E_n \left[ e^{\sigma M_{n + 1}} \left( \frac{1}{f(\sigma)} \right)^{n + 1} \right] \\
	&= \E_n \left[ e^{\sigma (M_n + X_{n + 1})} \left( \frac{1}{f(\sigma)} \right)^{n + 1} \right] \quad \text{(by definition of $M_{n + 1}$)} \\
	&= \E_n \left[ e^{\sigma M_n} \left( \frac{1}{f(\sigma)} \right)^n \left( \frac{1}{f(\sigma)} \right) e^{\sigma X_{n + 1}} \right] \\
	&= \E_n \left[ \frac{S_n}{f(\sigma)} e^{\sigma X_{n + 1}} \right] \quad \text{(by definition of $S_n$)} \\
	&= \frac{ S_n }{ f(\sigma) } \E_n \left[ e^{\sigma X_{n + 1}} \right] \quad \text{(adaptedness of $\frac{S_n}{f(\sigma)}$ to the first $n$ coin tosses)} \\
	&= \frac{ S_n }{ f(\sigma) } \left( pe^{\sigma \cdot (1)} + qe^{\sigma \cdot(-1)} \right) \quad \text{(by the independence lemma)} \\
	&= \frac{ S_n }{ f(\sigma) } f(\sigma) \\
	&= S_n
\end{align*}

\indent So $S_n$ remains a martingale for the case of $0 < p < \frac{1}{2}$. Now, again applying the Optional Sampling Theorem we have that the stopped process $S_{n\tau_1}$ must be a martingale. Hence
\begin{align*}
	\E_0 \left[ S_{n\land\tau_1} \right] &= \E_0 \left[ e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} \right] \\
	&= S_{0\land\tau_1} \quad \text{(by the martingale property)} \\
	&= S_0 \quad \text{(since $\tau_1 > 0$)} \\
	&= e^{\sigma M_0} \left( \frac{1}{f(\sigma)} \right)^0 \\
	&= 1
\end{align*}

\indent Suppose now that $\sigma > \sigma_0 = \log \frac{q}{p}$ from part {\bf (i)}. Then we have that $f(\sigma) > 1$. Therefore, $0 < \frac{1}{f(\sigma)} < 1$. From this fact we find
\begin{equation*}
	\lim_{n\to\infty} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} =
	\begin{cases}
		\left( \frac{1}{f(\sigma)} \right)^{\tau_1} & \text{if } \tau_1 < \infty \\
		0 & \text{if } \tau_1 = \infty
	\end{cases}
\end{equation*}

By the definition of $M_{n\land\tau_1}$ we note that
\begin{equation*}
	M_{n\land\tau_1} \leq 1
\end{equation*}

Hence
\begin{equation*}
	0 \leq e^{\sigma M_{n\land\tau_1}} \leq e^\sigma
\end{equation*}

Now, considering first $\tau_1 < \infty$
\begin{align*}
	\lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} &= \lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \lim_{n\to\infty} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} \\
	&= e^\sigma \left( \frac{1}{f(\sigma)} \right)^{\tau_1}
\end{align*}

and for the case of $\tau_1 = \infty$ we use our result that $e^{\sigma M_n}$ is bound above and below, so
\begin{align*}
	\lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} &= \lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \lim_{n\to\infty} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} \\
	&= \lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \cdot 0 \\
	&= 0
\end{align*}

Therefore, we may combine both cases as
\begin{equation*}
\lim_{n\to\infty} e^{\sigma M_{n\land\tau_1}} \left( \frac{1}{f(\sigma)} \right)^{n\land\tau_1} = \mathds 1_{\{ \tau_1 < \infty \}} e^\sigma \left( \frac{1}{f(\sigma)} \right)^{\tau_1}
\end{equation*}

\indent Now we wish to take the limit of the expectation of the stopped process $S_{n\land\tau_1}$ as $n \to \infty$. However, we have already shown that $\E_0[S_{n\land\tau_1}] = S_{0 \land \tau_1} = S_0 = 1$. So
\begin{align*}
	\lim_{n\to\infty} \E_0[S_{n\land\tau_1}] &= \lim_{n\to\infty} 1 \\
	&= 1
\end{align*}

Thus
\begin{align*}
	1 &= \lim_{n\to\infty} \E_0 \left[ S_{n\land\tau_1} \right] \\
	&= \E_0 \left[ \lim_{n\to\infty} S_{n\land\tau_1} \right] \quad \text{(Dominated Convergence)} \\
	&= \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} e^\sigma \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right]  \\
	\implies e^{-\sigma} &= \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] 
\end{align*}

Instead of taking taking the limit as $\sigma \downarrow 0$ as we did in {\bf (5.2)} we now consider the limit as $\sigma \downarrow \sigma_0$ since we have restricted $\sigma$ such that $\sigma > \sigma_0$. Taking this limit
\begin{align*}
	\lim_{\sigma\downarrow \sigma_0} e^{-\sigma} &= \lim_{\sigma\downarrow \sigma_0} \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	\implies e^{-\sigma_0} &= \lim_{\sigma \downarrow \sigma_0} \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	&= \E_0 \left[ \lim_{\sigma \downarrow \sigma_0} \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \quad \text{(Dominated Convergence)} \\
	&= \E_0 \left[ \mathds 1_{\{\tau_1 < \infty\}} \lim_{\sigma \downarrow \sigma_0} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] \\
	&= \E_0 \left[ \mathds 1_{\{\tau_1 < \infty \}} \right] \\
	&= \P \left( \{\tau_1 < \infty\} \right) 
\end{align*}

We may simplify this as
\begin{align*}
	\P \left( \{\tau_1 < \infty\} \right) &= e^{-\sigma_0} \\
	&= e^{-\log \frac{q}{p} } \\
	&= \frac{p}{q} 
\end{align*}

which is a permissible probability for this event since our definition of $p$ and $q$ satisfying $0 < p < \frac{1}{2}$ and $q = 1 - p$ implies that $0 < \frac{p}{q} < 1$. \\

%%
%% 5.3 (iii)
%%
{\bf (iii)} Let $\alpha \in (0,1)$. Repeating the arguments from {\bf (5.2.iv)} (since the preliminary steps are not affected by the requirement $0 < p < \frac{1}{2}$) we find that the $\sigma$ satisfying
\begin{equation*}
	\alpha = \frac{1}{f(\sigma)}
\end{equation*}

yielding
\begin{equation*}
	e^{-\sigma} = \frac{1 - \sqrt{1 - 4\alpha^2pq} }{ 2\alpha q }
\end{equation*}

From {\bf (5.3.ii)} we have that
\begin{equation*}
	e^{-\sigma} = \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \left( \frac{1}{f(\sigma)} \right)^{\tau_1} \right] 
\end{equation*}

Therefore, with our substitution of $\alpha = \frac{1}{f(\sigma)}$, we may write
\begin{equation*}
	\E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \alpha^{\tau_1} \right] = \frac{1 - \sqrt{1 - 4\alpha^2pq} }{ 2\alpha q }
\end{equation*}

Now, write
\begin{equation*}
	\E_0\left[ \alpha^{\tau_1} \right] = \E_0 \left[ \mathds 1_{\{ \tau_1 = \infty \}} \alpha^{\tau_1} \right] + \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \alpha^{\tau_1} \right]
\end{equation*}

but if $\tau_1 = \infty$ then, recalling $\alpha \in (0,1)$, we have $\mathds 1_{\{ \tau_1 = \infty \}} \alpha^{\tau_1} = 0$. Hence
\begin{align*}
	\E_0\left[ \alpha^{\tau_1} \right] &= \E_0 \left[ \mathds 1_{\{ \tau_1 = \infty \}} \alpha^{\tau_1} \right] + \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \alpha^{\tau_1} \right] \\
	&= \E_0 \left[ \mathds 1_{\{ \tau_1 < \infty \}} \alpha^{\tau_1} \right] \\
	&= \frac{1 - \sqrt{1 - 4\alpha^2pq} }{ 2\alpha q }
\end{align*}

as desired. \\

%%
%% 5.3 (iv)
%%
{\bf (iv)} Applying the Dominated Convergence Theorem we write
\begin{align*}
	\E \left[ \tau_1 \alpha^{\tau_1 - 1} \right] &= \frac{ \partial }{ \partial \alpha } \E \left[ \alpha^{\tau_1} \right] \\
	&= \frac{ \partial }{ \partial \alpha }  \frac{ 1 - \sqrt{1 - 4\alpha^2 pq } }{ 2\alpha q }  \quad \text{(from {\bf (5.3.iii)})} \\
	&= \frac{ 1 - \sqrt{1 - 4\alpha^2 pq} }{ 2\alpha^2 q \sqrt{1 - 4\alpha^2 pq} }
\end{align*}

Since we had defined $\alpha \in (0,1)$ we must take the limit as $\alpha \uparrow 1$
\begin{align*}
	\lim_{\alpha \uparrow 1} \E \left[ \tau_1 \alpha^{\tau_1 - 1} \right] &= \lim_{\alpha \uparrow 1} \frac{ 1 - \sqrt{1 - 4\alpha^2 pq} }{ 2\alpha^2 q \sqrt{1 - 4\alpha^2 pq} } \\
	&= \frac{ 1 - \sqrt{1 - 4 pq} }{ 2 q \sqrt{1 - 4 pq} } \\
	&= \frac{ 1 - \sqrt{1 - 4 p ( 1 - p ) } }{ 2 (1 - p) \sqrt{1 - 4 p (1 - p)} } \\
	&= \frac{ 1 - \sqrt{ 1 - 4p + 4p^2 }}{ 2 (1 - p) \sqrt{ 1 - 4p + 4p^2 } } \\
	&= \frac{ 1 - \sqrt{(1 - 2p)^2} }{ 2(1 - p) \sqrt{ (1 - 2p)^2 } } \\
	&= \frac{1 - |1 - 2p| }{ 2(1 - p)|1 - 2p| }
\end{align*}

Now, since $0 < p < \frac{1}{2}$ we have that $1 - 2p > 0$, thus
\begin{align*}
	\lim_{\alpha \uparrow 1} \E \left[ \tau_1 \alpha^{\tau_1 - 1} \right] &= \frac{1 - (1 - 2p) }{ 2(1 - p)(1 - 2p) } \\
	&= \frac{p}{(1 - p)(1 - 2p)} \\
	&= \frac{p}{q(1 - p - p)} = \frac{p}{q(q - p)}
\end{align*}

as desired. \\

\newpage
%%
%% 5.6 
%%
{\bf Solution 5.6}: $S_0 = 4, K = 4, u = 2, d = \frac{1}{2}, r = \frac{1}{4}$. Show that an American put expiring at time $N = 1$ has price $V_0 = 0.8$, expiring at time $N = 3$ has price $V_0 = 0.928$, and expiring at time $N = 5$ has price $V_0 = 0.96896$.

\newpage
%%
%% 5.7 
%%
{\bf Solution 5.7}: 
%%
%% 5.7 (i)
%%
{\bf (i)} Recall that we had found 
\begin{equation*}
	v(s) = 
	\begin{cases}
		4 - s & \text{if } s \leq 2 \\
		\frac{4}{s} & \text{if } s \geq 4
	\end{cases}
\end{equation*}

so with $s = 2^j$ we have
\begin{equation*}
	v(2^j) = 
	\begin{cases}
		4 - 2^j & \text{if } j \leq 1 \\
		\frac{4}{2^j} & \text{if } j \geq 2
	\end{cases}
\end{equation*}

Therefore, for $j \leq 0$, we find
\begin{align*}
	c(2^j) &= v(2^j) - \frac{4}{5} \left[\frac{1}{2} v(2\cdot 2^j) + \frac{1}{2} v(\frac{1}{2} 2^j) \right] \\
	&= (4 - 2^j) - \frac{2}{5} \left[ (4 - 2^{j + 1}) + (4 - 2^{j - 1}) \right] \\
	&= 4 - 2^j - \frac{2}{5}\left[ 8 - 2^{j - 1}(4 + 1)\right] \\
	&= 4 - 2^j - \frac{2}{5}\left[ 8 - 5\cdot2^{j - 1}\right] \\
	&= 4 - 2^j - \frac{16}{5} + 2^j \\
	&= \frac{4}{5}
\end{align*}

For $j = 1$ we find
\begin{align*}
	c(2^j) &= v(2^j) - \frac{4}{5} \left[\frac{1}{2} v(2\cdot 2^j) + \frac{1}{2} v(\frac{1}{2} 2^j) \right] \\
	&= v(2) - \frac{2}{5} \left[ v(4) + v(1) \right] \\
	&= (4 - 2) - \frac{2}{5} \left[ \frac{4}{4} + (4 - 1) \right] \\
	&= 2 - \frac{2}{5}\left[ 1 + 3 \right] \\
	&= 2 - \frac{8}{5} \\
	&= \frac{2}{5}
\end{align*}

and for $j \geq 2$
\begin{align*}
	c(2^j) &= v(2^j) - \frac{4}{5} \left[\frac{1}{2} v(2\cdot 2^j) + \frac{1}{2} v(\frac{1}{2} 2^j) \right] \\
	&= \frac{4}{2^j} - \frac{2}{5} \left[ \frac{4}{2^{j + 1}} + \frac{4}{2^{j - 1}} \right] \\
	&= \frac{4}{2^j} - \frac{2}{5} \frac{4}{2^{j - 1}} \left[ \frac{1}{4} + 1 \right] \\
	&= \frac{4}{2^j} - \frac{8}{5\cdot 2^{j - 1}} \frac{5}{4} \\
	&= \frac{4}{2^j} - \frac{2}{2^{j - 1}} \\
	&= \frac{4}{2^j} - \frac{4}{2^j} \\
	&= 0
\end{align*}


%%
%% 5.7 (ii)
%%
{\bf (ii)} For $j \leq 0$ we find
\begin{align*}
	\delta(2^j) &= \frac{ v( 2^{j + 1} )  - v( 2^{j - 1} ) }{ 2^{j + 1} - 2^{j - 1} } \\
	&= \frac{ (4 - 2^{j + 1} ) - (4 - 2^{j - 1}) }{ 2^{j + 1} - 2^{j - 1} } \\
	&= \frac{ -(2^{j + 1} - 2^{j - 1} ) }{ 2^{j + 1} - 2^{j - 1} } \\
	&= -1
\end{align*}

for $j = 1$
\begin{align*}
	\delta(2^j) &= \frac{ v( 2^{j + 1} )  - v( 2^{j - 1} ) }{ 2^{j + 1} - 2^{j - 1} } \\
	&= \frac{ v(4) - v(1) }{ 4 - 1 } \\
	&= \frac{ \frac{4}{4} - (4 - 1) }{ 3} \\
	&= -\frac{2}{3}
\end{align*}

and for $j \geq 2$
\begin{align*}
	\delta(2^j) &= \frac{ v( 2^{j + 1} )  - v( 2^{j - 1} ) }{ 2^{j + 1} - 2^{j - 1} } \\
	&= \frac{ \frac{4}{2^{j + 1}} - \frac{4}{2^{j - 1}} }{ 2^{j + 1} - 2^{j - 1} } \\
	&= \frac{4}{2^{j - 1}} \cdot \frac{ \frac{1}{4} - 1 }{ 2^{j + 1} - 2^{j - 1} } \\
	&= -\frac{4}{2^{j - 1}} \cdot \frac{ \frac{3}{4} }{ 3 \cdot 2^{j - 1} } \\
	&= - \frac{1}{2^{j - 1} \cdot 2^{j - 1}} \\
	&= -\frac{1}{2^{2j - 2}} \\
	&= -\frac{1}{2^{2j}2^{-2}} \\
	&= -\frac{4}{2^{2j}}
\end{align*}

%%
%% 5.7 (iii)
%%
{\bf (iii)}

\newpage
%%
%% 5.8
%%
{\bf Solution 5.8}:

%%
%% 5.8 (i)
%%
{\bf (i)}

%%
%% 5.8 (ii)
%%
{\bf (ii)}

%%
%% 5.8 (iii)
%%
{\bf (iii)}

%%
%% 5.8 (iv)
%%
{\bf (iv)}




















\end{document}
